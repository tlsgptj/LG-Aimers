{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN+oA81xPnxP1miT2jWlLE2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsgptj/LG-Aimers/blob/main/LGAimers5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n6RKu0EimqHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼파라미터 튜닝:\n",
        "\n",
        "GridSearchCV를 사용하여 n_estimators, max_depth, min_samples_split, min_samples_leaf, class_weight 등의 하이퍼파라미터를 최적화\n",
        "데이터 불균형 처리:\n",
        "\n",
        "SMOTE를 사용하여 Normal 클래스의 샘플을 증대시켜 데이터 불균형 문제를 완화\n",
        "스케일링 적용:\n",
        "\n",
        "StandardScaler를 사용하여 데이터의 스케일을 정규화\n",
        "교차 검증 사용:\n",
        "\n",
        "StratifiedKFold를 사용하여 더욱 견고한 성능 평가를 수행\n",
        "클래스 가중치:\n",
        "\n",
        "class_weight를 사용하여 불균형 데이터를 처리할 때 가중치를 부여"
      ],
      "metadata": {
        "id": "eRn2jcXnmlaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE  # Imbalanced-learn 라이브러리 사용\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "ROOT_DIR = \"data\"\n",
        "RANDOM_STATE = 110\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
        "normal_ratio = 1.0  # 1.0 means 1:1 ratio\n",
        "\n",
        "df_normal = train_data[train_data[\"target\"] == \"Normal\"]\n",
        "df_abnormal = train_data[train_data[\"target\"] == \"AbNormal\"]\n",
        "\n",
        "num_normal = len(df_normal)\n",
        "num_abnormal = len(df_abnormal)\n",
        "print(f\"  Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
        "\n",
        "# Oversampling으로 비율 맞추기\n",
        "if num_normal < num_abnormal:\n",
        "    smote = SMOTE(sampling_strategy='auto', random_state=RANDOM_STATE)\n",
        "    df_normal, _ = smote.fit_resample(df_normal, np.zeros(len(df_normal)))\n",
        "\n",
        "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=True, random_state=RANDOM_STATE)\n",
        "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
        "df_concat.value_counts(\"target\")\n",
        "df_train, df_val = train_test_split(\n",
        "    df_concat,\n",
        "    test_size=0.3,\n",
        "    stratify=df_concat[\"target\"],\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "\n",
        "def print_stats(df: pd.DataFrame):\n",
        "    num_normal = len(df[df[\"target\"] == \"Normal\"])\n",
        "    num_abnormal = len(df[df[\"target\"] == \"AbNormal\"])\n",
        "\n",
        "    print(f\"  Total: Normal: {num_normal}, AbNormal: {num_abnormal}\" + f\" ratio: {num_abnormal/num_normal}\")\n",
        "\n",
        "\n",
        "# Print statistics\n",
        "print(f\"  \\tAbnormal\\tNormal\")\n",
        "print_stats(df_train)\n",
        "print_stats(df_val)\n",
        "\n",
        "# RandomForestClassifier와 파이프라인 설정\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', RandomForestClassifier(random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "# 하이퍼파라미터 그리드 설정\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200, 300],\n",
        "    'model__max_depth': [None, 10, 20, 30],\n",
        "    'model__min_samples_split': [2, 5, 10],\n",
        "    'model__min_samples_leaf': [1, 2, 4],\n",
        "    'model__class_weight': [None, 'balanced']  # 클래스 가중치 추가\n",
        "}\n",
        "\n",
        "# 그리드 서치 설정\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    scoring='f1_macro'\n",
        ")\n",
        "\n",
        "# 특성 선택\n",
        "features = []\n",
        "\n",
        "for col in df_train.columns:\n",
        "    if col != \"target\":\n",
        "        try:\n",
        "            df_train[col] = df_train[col].astype(int)\n",
        "            features.append(col)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "train_x = df_train[features]\n",
        "train_y = df_train[\"target\"]\n",
        "\n",
        "# 모델 학습\n",
        "grid_search.fit(train_x, train_y)\n",
        "\n",
        "# 최적 하이퍼파라미터 출력\n",
        "print(\"Best hyperparameters:\")\n",
        "pprint(grid_search.best_params_)\n",
        "\n",
        "# 검증 데이터 예측\n",
        "val_x = df_val[features]\n",
        "val_y = df_val[\"target\"]\n",
        "\n",
        "val_pred = grid_search.predict(val_x)\n",
        "\n",
        "# 성능 평가\n",
        "print(\"Validation Results:\")\n",
        "print(classification_report(val_y, val_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(val_y, val_pred))\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\n",
        "df_test_x = test_data[features]\n",
        "\n",
        "for col in df_test_x.columns:\n",
        "    try:\n",
        "        df_test_x.loc[:, col] = df_test_x[col].astype(int)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "test_pred = grid_search.predict(df_test_x)\n",
        "\n",
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "acqKPnKrmsBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.over_sampling import SMOTE  # Imbalanced-learn 라이브러리 사용\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "ROOT_DIR = \"data\"\n",
        "RANDOM_STATE = 110\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
        "normal_ratio = 1.0  # 1.0 means 1:1 ratio\n",
        "\n",
        "# 타겟 변수 레이블을 0과 1로 변환\n",
        "train_data['target'] = train_data['target'].map({'Normal': 1, 'AbNormal': 0})\n",
        "\n",
        "df_normal = train_data[train_data[\"target\"] == 1]\n",
        "df_abnormal = train_data[train_data[\"target\"] == 0]\n",
        "\n",
        "num_normal = len(df_normal)\n",
        "num_abnormal = len(df_abnormal)\n",
        "print(f\"  Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
        "\n",
        "# Oversampling으로 비율 맞추기\n",
        "if num_normal < num_abnormal:\n",
        "    smote = SMOTE(sampling_strategy='auto', random_state=RANDOM_STATE)\n",
        "    df_normal, _ = smote.fit_resample(df_normal, np.zeros(len(df_normal)))\n",
        "\n",
        "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=True, random_state=RANDOM_STATE)\n",
        "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
        "df_concat.value_counts(\"target\")\n",
        "df_train, df_val = train_test_split(\n",
        "    df_concat,\n",
        "    test_size=0.3,\n",
        "    stratify=df_concat[\"target\"],\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "\n",
        "def print_stats(df: pd.DataFrame):\n",
        "    num_normal = len(df[df[\"target\"] == 1])\n",
        "    num_abnormal = len(df[df[\"target\"] == 0])\n",
        "\n",
        "    print(f\"  Total: Normal: {num_normal}, AbNormal: {num_abnormal}\" + f\" ratio: {num_abnormal/num_normal}\")\n",
        "\n",
        "\n",
        "# Print statistics\n",
        "print(f\"  \\tAbnormal\\tNormal\")\n",
        "print_stats(df_train)\n",
        "print_stats(df_val)\n",
        "\n",
        "# 범주형 열과 수치형 열 구분\n",
        "categorical_cols = df_train.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = df_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# 특성 선택 및 데이터 타입 확인\n",
        "features = [col for col in df_train.columns if col != 'target']\n",
        "\n",
        "train_x = df_train[features]\n",
        "train_y = df_train[\"target\"]\n",
        "\n",
        "# 결측값 및 무한값 처리\n",
        "train_x = train_x.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "# 전처리 파이프라인 설정\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# XGBClassifier와 파이프라인 설정\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', XGBClassifier(\n",
        "        random_state=RANDOM_STATE,\n",
        "        use_label_encoder=False,  # 경고 메시지 방지\n",
        "        eval_metric='logloss'     # 평가 지표 설정\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 하이퍼파라미터 그리드 설정\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200, 300],\n",
        "    'model__max_depth': [3, 5, 7, 10],\n",
        "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "    'model__subsample': [0.8, 0.9, 1.0],\n",
        "    'model__colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'model__scale_pos_weight': [1, num_abnormal/num_normal]  # 클래스 가중치 추가\n",
        "}\n",
        "\n",
        "# 그리드 서치 설정\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "    n_jobs=-1,\n",
        "    verbose=3,  # verbose를 높여 더 많은 정보 출력\n",
        "    scoring='f1_macro',\n",
        "    error_score='raise'  # 디버깅을 위해 에러 발생 시 예외를 발생\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "try:\n",
        "    grid_search.fit(train_x, train_y)\n",
        "    # 최적 하이퍼파라미터 출력\n",
        "    print(\"Best hyperparameters:\")\n",
        "    pprint(grid_search.best_params_)\n",
        "except ValueError as e:\n",
        "    print(f\"Error during grid search: {e}\")\n",
        "\n",
        "# 검증 데이터 예측\n",
        "val_x = df_val[features]\n",
        "val_y = df_val[\"target\"]\n",
        "\n",
        "# 결측값 및 무한값 처리\n",
        "val_x = val_x.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "val_pred = grid_search.predict(val_x)\n",
        "\n",
        "# 성능 평가\n",
        "print(\"Validation Results:\")\n",
        "print(classification_report(val_y, val_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(val_y, val_pred))\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\n",
        "df_test_x = test_data[features]\n",
        "\n",
        "# 범주형 열과 수치형 열 구분\n",
        "df_test_x = df_test_x.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "test_pred = grid_search.predict(df_test_x)\n",
        "\n",
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Fr6BciXNs4vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LightGBM\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE  # Imbalanced-learn 라이브러리 사용\n",
        "\n",
        "ROOT_DIR = \"data\"\n",
        "RANDOM_STATE = 110\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
        "normal_ratio = 1.0  # 1.0 means 1:1 ratio\n",
        "\n",
        "df_normal = train_data[train_data[\"target\"] == \"Normal\"]\n",
        "df_abnormal = train_data[train_data[\"target\"] == \"AbNormal\"]\n",
        "\n",
        "num_normal = len(df_normal)\n",
        "num_abnormal = len(df_abnormal)\n",
        "print(f\"  Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
        "\n",
        "# Oversampling으로 비율 맞추기\n",
        "if num_normal < num_abnormal:\n",
        "    smote = SMOTE(sampling_strategy='auto', random_state=RANDOM_STATE)\n",
        "    df_normal, _ = smote.fit_resample(df_normal, np.zeros(len(df_normal)))\n",
        "\n",
        "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=True, random_state=RANDOM_STATE)\n",
        "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
        "df_concat.value_counts(\"target\")\n",
        "df_train, df_val = train_test_split(\n",
        "    df_concat,\n",
        "    test_size=0.3,\n",
        "    stratify=df_concat[\"target\"],\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "\n",
        "def print_stats(df: pd.DataFrame):\n",
        "    num_normal = len(df[df[\"target\"] == \"Normal\"])\n",
        "    num_abnormal = len(df[df[\"target\"] == \"AbNormal\"])\n",
        "\n",
        "    print(f\"  Total: Normal: {num_normal}, AbNormal: {num_abnormal}\" + f\" ratio: {num_abnormal/num_normal}\")\n",
        "\n",
        "\n",
        "# Print statistics\n",
        "print(f\"  \\tAbnormal\\tNormal\")\n",
        "print_stats(df_train)\n",
        "print_stats(df_val)\n",
        "\n",
        "# LGBMClassifier와 파이프라인 설정\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LGBMClassifier(random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "# 하이퍼파라미터 그리드 설정\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200, 300],\n",
        "    'model__max_depth': [3, 5, 7, -1],  # -1 for no limit\n",
        "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "    'model__num_leaves': [31, 63, 127],  # Larger num_leaves -> more complex model\n",
        "    'model__boosting_type': ['gbdt', 'dart'],  # Different boosting methods\n",
        "    'model__class_weight': [None, 'balanced']  # 클래스 가중치 추가\n",
        "}\n",
        "\n",
        "# 그리드 서치 설정\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    scoring='f1_macro'\n",
        ")\n",
        "\n",
        "# 특성 선택\n",
        "features = []\n",
        "\n",
        "for col in df_train.columns:\n",
        "    if col != \"target\":\n",
        "        try:\n",
        "            df_train[col] = df_train[col].astype(float)  # LightGBM prefers float\n",
        "            features.append(col)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "train_x = df_train[features]\n",
        "train_y = df_train[\"target\"]\n",
        "\n",
        "# 모델 학습\n",
        "grid_search.fit(train_x, train_y)\n",
        "\n",
        "# 최적 하이퍼파라미터 출력\n",
        "print(\"Best hyperparameters:\")\n",
        "pprint(grid_search.best_params_)\n",
        "\n",
        "# 검증 데이터 예측\n",
        "val_x = df_val[features]\n",
        "val_y = df_val[\"target\"]\n",
        "\n",
        "val_pred = grid_search.predict(val_x)\n",
        "\n",
        "# 성능 평가\n",
        "print(\"Validation Results:\")\n",
        "print(classification_report(val_y, val_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(val_y, val_pred))\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\n",
        "df_test_x = test_data[features]\n",
        "\n",
        "for col in df_test_x.columns:\n",
        "    try:\n",
        "        df_test_x.loc[:, col] = df_test_x[col].astype(float)  # LightGBM prefers float\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "test_pred = grid_search.predict(df_test_x)\n",
        "\n",
        "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
        "df_sub = pd.read_csv(\"submission.csv\")\n",
        "df_sub[\"target\"] = test_pred\n",
        "\n",
        "# 제출 파일 저장\n",
        "df_sub.to_csv(\"submission.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "tt4X5NtzJxQ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}